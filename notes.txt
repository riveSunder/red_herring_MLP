
# I wrote a 7 layer fully connected classifier with flags for various hyperparameters and for specifying a logdir for tensorboard
# Three datasets are available (also specified via flags): iris, wine quality, and digits, all from scikit-learn datasets

# Added summary ops for tensorboard visualizations. 
# I've gather that the merge_all op should follow all of the summary ops in the graph definition. It won't do to define the merge op in the training loop or else it will write new events for each iteration (appended with _number).
# However the summary writer op is called in the training loop.

# With how I've built it so far, I can't log both training and validation accuracy/loss, because the summary ops are defined (and named) in the graph and the summary writer is called from the training loop. Therefore, everything that is logged is from validation runs. 


# Using the tf.layers and tf.contrib.learn, I wrote another, more abstraced MLP model for classification. This one has better performance than the low-level model using tf.matmul operations. 
